---
title: "Topic Modeling"
author: "JMFR"
date: "13/5/2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introducción

Sopongamos que en un archivo se ha encontrado un legajo con más de cuatrocientas cincuenta páginas de texto que parecen tratar de pensadores porque al trascribirlas se han topado recurrentemente con Freud, Voltaire, Chomsky y Maquiavelo y términos como lenguaje, lingüística, política, revolución, política, sociedad, crítica, análisis, social, historia, príncipe, moral, ideas, psicoanálisis, etc. Tan solo han sido capaces de dibujar una nube de palabras y quieres saber si se pueden agrupar por temas ya que esos cuatro personajes que aparecen recurrentemente dan la pista de que podría tratarse de cuatro capítulos en los que se habla de esos autores.

```{R echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
# Carga la lista de stopwords españolas
vacias <- as_tibble(read_delim("https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/diccionarios/vacias.txt",
                               delim = "\t",
                               col_names = TRUE,
                               quote = "\"",
                               locale = default_locale()))
# Los textos los cargas desde un repositorio externo
url <- "https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/filosofos/"
ficheros <- c("filosofo1.txt", "filosofo2.txt", "filosofo3.txt", "filosofo4.txt")
ensayos <- tibble()
# Lee los texto y los divide en "páginas"
for (j in 1:length(ficheros)){
  texto.entrada <- as_tibble(read_lines(paste(url, ficheros[j], sep = ""),
                              locale = default_locale()))
  ensayos <- bind_rows(ensayos, texto.entrada)
}
palabras <- ensayos %>%
  unnest_tokens(palabra, value) %>%
  anti_join(vacias) %>%
  count(palabra, sort = T) %>%
  with(wordcloud(palabra, n, max.words = 100, color = brewer.pal(8,"Dark2")))
```

Hay una técnica procedente de la inteligencia artificial (IA), del subcampo del aprendizaje automático (_machine learning_), que puede ser de gran ayuda para clasificar estos textos. Es el llamado _topic modelling_, que lo que pretende es identificar, sin la ayuda de ningún diccionario, los temas (_tópicos_) principales que encierra un texto. Por ejemplo, examina las siguientes nubes de palabras de cuatro novelas españolas e intenta establecer de qué trata cada una de ellas.

```{R echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer) # Puede que la cargue el anterior
vacias <- as_tibble(read_delim("datos/diccionarios/vacias.txt",
                               delim = "\t",
                               col_names = TRUE,
                               quote = "\""))
novela <- read_lines("datos/Topic-Nubes/Ritos-Muerte.txt")
texto <- tibble(texto = novela)
palabras <- texto %>%
  unnest_tokens(palabra, texto) %>%
  anti_join(vacias) %>%
  count(palabra, sort = T) %>%
  with(wordcloud(palabra, n, max.words = 100, color = brewer.pal(8,"Dark2")))

novela <- read_lines("datos/Topic-Nubes/Edades-Lulu.txt")
texto <- tibble(texto = novela)
palabras <- texto %>%
  unnest_tokens(palabra, texto) %>%
  anti_join(vacias) %>%
  count(palabra, sort = T) %>%
  with(wordcloud(palabra, n, max.words = 100, color = brewer.pal(8,"Dark2")))

novela <- read_lines("datos/Topic-Nubes/Trafalgar.txt")
texto <- tibble(texto = novela)
palabras <- texto %>%
  unnest_tokens(palabra, texto) %>%
  anti_join(vacias) %>%
  count(palabra, sort = T) %>%
  with(wordcloud(palabra, n, max.words = 100, color = brewer.pal(8,"Dark2")))

novela <- read_lines("datos/Topic-Nubes/Ardor-guerrero.txt")
texto <- tibble(texto = novela)
palabras <- texto %>%
  unnest_tokens(palabra, texto) %>%
  anti_join(vacias) %>%
  count(palabra, sort = T) %>%
  with(wordcloud(palabra, n, max.words = 100, color = brewer.pal(8,"Dark2")))
```

La primera es una novela policíaca (_comisario_, _subinspector_, _policía_, _caso_ y que el delito parece ser una violación por la aparición del término _violador_). En la segunda abundan las referencias a diversas partes del cuerpo (_piernas_, _lengua_, _brazos_, _ojos_, _boca_, _dedos_, _cabeza_, _labios_, _cara_…), pero estas palabras por sí solas no constituyen un tópico, pueden aparecer en muchos otros tipos de textos; _cabeza_ y _ojos_, por ejemplo, aparecen en las cuatro nubes, pero la ocurrencia de _cama_ y _sexo_ permiten restringir el tema y, podría ser una novela rosa (o erótica). La tercera parece que se trata de una batalla naval (_navío_, _escuadra_, _buque_, _combate_, _muerte_, _guerra_, _mar_, _artillería_, _cañones_, _marineros_) en la que están involucrados los ingleses. La última parece situarse también en un ambiente militar, pero infinítamente más tranquilo, en la vida de cuartel (_capitán_, _uniforme_, _reclutas_, _botas_, _ejército_, _militares_, _sargento_, _campamento_) durante la llamada _mili_, es decir, el servicio militar obligatorio.

Lo mismo que acabas de hacer para ver de qué tratan esas novelas, pero has tenido que jugar con tu conocimiento del mundo, con un amplio repertorio léxico, puede hacerlo una máquina que no sabe nada de español, o para el caso de ninguna lengua, pues para ella todo son ceros y unos.

Las cuatro novelas procesadas y representadas en las cuatro nuebes de palabras anteriores son:

1. _Ritos de muerte_, de Alicia-Gimenez-Bartlet,
2. _Las edades de Lulú_, de Almudena Grandes,
3. _Trafalgar_, de Benito Pérez Galdós y
4. _Ardor guerrero_, de Antonio Muñiz Molina.

El proceso matemático que hay tras el modelado de tópicos, como en casi todo lo que estas viendo, es tremendamente complejo, pero el procedimiento, a grandes rasgos es bastante sencillo de entender.

Todo texto presenta un abanico de tópicos y esos tópicos se expresan por medio de palabras, en especial sustantivos, lo único que tiene que hacer la máquina es contar las palabras y ver cuáles coocurren, algo que hay has visto, con cuáles y después el investigador debe decidir cuáles son los verdaderos tópicos, pues no todos son tan sencillos de decidir como los que te he mostrado en las nubes anteriores.

El ordenador al ejecutar un algoritmo de modelado de tópicos, de los que hay varios disponibles –LDA, Mallet–, devuelve dos tipos de datos. Por una parte, informa de qué tópicos existen en la colección de textos y qué palabras los conforman. Por la otra, informa de la proporción que hay de cada tópico en cada texto. En la figura siguiente te avanzo el resultado de analizar con `R` los folios del legajo.

```{R echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidytext) 
library(tm)
library(topicmodels)
library(scales)

# Carga la lista de stopwords españolas
vacias <- as_tibble(read_delim("https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/diccionarios/vacias.txt",
                               delim = "\t",
                               col_names = TRUE,
                               quote = "\"",
                               locale = default_locale()))
# Los textos los cargas desde un repositorio externo
url <- "https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/filosofos/"
titulos <- c("Chomsky", "Freud", "Maquiavelo", "Voltaire")
ficheros <- c("filosofo1.txt", "filosofo2.txt", "filosofo3.txt", "filosofo4.txt")
ensayos <- tibble(texto = character(),
                      titulo = character(),
                      pagina = numeric())
# Lee los texto y los divide en "páginas"
for (j in 1:length(ficheros)){
texto.entrada <- read_lines(paste(url, ficheros[j], sep = ""),
                            locale = default_locale())
texto.todo <- paste(texto.entrada, collapse = " ")
por.palabras <- strsplit(texto.todo, " ")
texto.palabras <- por.palabras[[1]]
trozos <- split(texto.palabras,
                ceiling(seq_along(texto.palabras)/375))
for (i in 1:length(trozos)){
  fragmento <- trozos[i]
  fragmento.unido <- data_frame(texto = paste(unlist(fragmento),
                                              collapse = " "),
                                titulo = titulos[j],
                                pagina = i)
  ensayos <- bind_rows(ensayos, fragmento.unido)
}
}

# Borra todas los objetos que no serán necesarios
rm(ficheros, titulos, trozos, fragmento, url, fragmento.unido, texto.entrada, texto.palabras, texto.todo, por.palabras, i, j)

# Divide (tokeniza) en palabras por capítulo
por_pagina_palabras <- ensayos %>%   
  unite(titulo_pagina, titulo, pagina) %>%                 
  unnest_tokens(palabra, texto)

# Elimina palabras vacías
palabra_conteo <- por_pagina_palabras %>%   
  anti_join(vacias) %>%   
  count(titulo_pagina, palabra, sort = TRUE) %>%   
  ungroup()

paginas_dtm <- palabra_conteo %>%
  cast_dtm(titulo_pagina, palabra, n)

paginas_lda <- LDA(paginas_dtm, k = 4, control = list(seed = 1234))

paginas_lda_td <- tidy(paginas_lda, matrix = "beta")

terminos_frecuentes <- paginas_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

terminos_frecuentes %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

## Al teclado
Lo que vas a realizar en adelante es comprobar que el ordenador es capaz de establecer que en esas supuestas 364 páginas proceden de cuatro libros independientes, que bien podrían ser capítulos de un libro extenso sobre pensadores occidentales. Como de antemano sabes cuál ha de ser el resultado, te servirá para ver la bondad del procedimiento. Es enredado en cuanto a qué es lo que hace la máquina en cada momento, pero se puede seguir y replicar con sencillez.

```{R echo = FALSE}
rm(list = ls())
```

Vas a necesitar tres nuevas librerías `tm`, `topicmodels` y `scales`[^1]. Ya sabes cómo instalarlas:

```{R eval = FALSE}
install.packages("tm")
install.packages("topicmodels")
install.packages("scales")
```

Cárgalas junto con `tidyverse` y `tidytext`.

```{R}
library(tidyverse)
library(tidytext)
library(tm)
library(topicmodels)
library(scales)
```

Lo siguiente es cargar la lista de palabras vacías para borrar todas las palabras de función que no aportarán nada al establecimiento de los tópicos.

```{R eval = FALSE}
vacias <- as_tibble(read_delim("https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/diccionarios/vacias.txt",
                               delim = "\t",
                               col_names = TRUE,
                               quote = '"',
                               locale = default_locale()))
```

Te aclaro lo que hace la línea. Lee con `read_delim()` un fichero que se encuentra en un servidor externo, de ahí el larguísimo argumento. Aunque es una texto `.txt` en realida es un tabla en las que las diversas columnas están separadas por tabuladores `delim = "\t"`; las columnas tienen nombres `col_names = TRUE`; las palabras están encerradas entre comillas `quote = '"'`; y, por último, `locale = default_locale()` para evitar los desesperantes problemas que Windows tienes con la letras portadoras de diacríticos. Todo la información que baje la convertira en una `tibble`con `as_tibble()` y lo guardará en un vector llamado `vacias`.

Al ejecutar la orden anterior salrá este mensaje
```{R echo = FALSE}
vacias <- as_tibble(read_delim("https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/diccionarios/vacias.txt",
                               delim = "\t",
                               col_names = TRUE,
                               quote = '"',
                               locale = default_locale()))
```
Esto quiere decir que se ha cargado correctamente la tabla `vacias` y que consta de una sola columna llamada `palabra` cuyo contendio son caracteres (`col_charater()`).

Los cuatro textos con los que vas a trabajar son cuatro ensayos sobre Chomsky, Freud, Maquiavelo, Marx y Voltaire. Crea un vector con estos nombres y llámalo titulo.

```{R}
titulos <- c("Chomsky", "Freud", "Maquiavelo", "Voltaire")
```

pero no vas a saber cuál es cuál, para mantener un poco la ficción. Cada uno de los textos que está escondido tras los nombres de fichero `filosofo1.txt` a `filosofo4.txt` que tienes en el vector `ficheros`
```{R}
ficheros <- c("filosofo1.txt", "filosofo2.txt", "filosofo3.txt", "filosofo4.txt")
```

Pero como están en un servidor externo, tienes que indicar la `url` donde las debe buscar

```{R}
url <- "https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/filosofos/"
```

Lo siguiente es crear la tabla `ensayos` en la que se guardarán los cuatro textos divididos en páginas:

```{R}
ensayos <- tibble(texto = character(),
                  titulo = character(),
                  pagina = numeric())
```

Lo que sigue es el proceso de carga de los cuatro textos y su subdivisión en páginas, las que, supuestamente, encontraste en el archivo. Para hacerlo, vas a utilizar dos bucles for anidados uno dentro de otro. Esto ya es un poco más complicado, pero ya estás aproximándote al final del curso. En la primera parte del bucle, leerás cada uno de los textos. Los convertirás en una sola cadena de caracteres, lo cual te permitirá dividirlo en palabras conservando la puntuación, y después podrás repartirlo en secciones de 375 palabras (he calculado que es la media de palabras por página con varias novelas, pero nada te impide usar cualquier otro valor). Cada una de esas secciones es cada una de las casi 370 páginas ficticias que encontraste en el legajo. Después, volverás a unir las palabras en un solo elemento y por último guardarás cada uno de esos trozos (páginas) dentro de ensayos. Numerarás cada uno de ellos y le darás un identificador. Es mucho lo que tienes que hacer. Copia estas líneas de código[3]:

```{R}
for (j in 1:length(ficheros)){
  texto.entrada <- read_lines(paste(url, ficheros[j], sep = ""),
                              locale = default_locale())
  texto.todo <- paste(texto.entrada, collapse = " ")
  por.palabras <- strsplit(texto.todo, " ")
  texto.palabras <- por.palabras[[1]]
  trozos <- split(texto.palabras,
                  ceiling(seq_along(texto.palabras)/375))
  for (i in 1:length(trozos)){
    fragmento <- trozos[i]
    fragmento.unido <- data_frame(texto = paste(unlist(fragmento),
                                                collapse = " "),
                                  titulo = titulos[j],
                                  pagina = i)
    ensayos <- bind_rows(ensayos, fragmento.unido)
  }
}
```

### Explicación del código
El bucle externo es el encargado de leer los textos y dividirlos en segmentos de 375 palabras que conformarán cada una de los folios que encontraste. Este está regido por la longitud del vector `ficheros` –`length(ficheros)`– y la variable de control es `j`.

En la primera línea leerá un texto `ficheros[j]` y lo guardará en `texto.entrada`.
Una vez que tienes en `texto.entrada` uno de los ficheros, en la siguiente línea lo que haces es guardar en `texto.todo` una copia del fichero que acabas de leer, pero en una sola cadena de caracteres. Para hacerlo le has dicho que pegue con `paste()` todos los elementos que haya en `texto.entrada` y que el pegamento sea un espacio en blanco. Como no se trata de separar piezas, sino de unir, usas `collapse = " "`.

A continuación, lo vas a dividir en palabras sueltas y lo vas a guardar en `por.palabras`. No es lo mismo que lo que has hecho, y volverás hacer con `unnest_tokens()`. Esta vez solo lo vas a dividir sin tocar para nada cada una de las palabras, lo que conservará, además, todos los signos de puntuación y la distinción de entre mayúsculas y minúsculas. La función de dividir un vector la realiza la función `strsplit` (_string split_ = 'dividir cadena'), y para que sepa dónde están los márgenes, le indicas, entre comillas, que es un espacio en blanco. Pero esta función crea una lista, que son difíciles de manejar, por lo que extraes a un vector de caracteres el contenido de la lista `por.palabras` de una manera un tanto peculiar, con el doble corchete `[[]]`. Como solo tiene un vector, le decimos que lo extraiga con `por.palabras[[1]]` y lo guardarás en `texto.palabras`. Ahora hay que dividirlo en secciones –`trozos`– de 375 palabras. Sé que es de locos lo que estás haciendo. Unes, cortas, divides… y aún te falta volverlo a unir.

Para dividir ahora `texto.palabras` en trozos utilizarás la función `split` (= 'dividir'). Esta requiere saber qué vector es el que ha de dividir, primer argumento, y cómo lo debe dividir, segundo argumento. El primero lo tienes claro, es `texto.palabras`. El segundo es un cálculo un tanto curioso, pero muy elemental. Le pides que cuente cuantos elementos tiene el vector que quieres dividir –`texto.palabras`– con `seq_along()` y que lo divida entre 375, que es el número de palabras que quieres contemplar para cada página. Como las probabilidades de que dé un número exacto de porciones de 375 es remoto, con `ceiling()` lo que haces el decirle, que lo que quede, el resto, lo meta en el último trozo, sin importar si son 375 o 25 palabras. Por ejemplo, el último texto tiene 38690 palabras, si lo divides entre 375 el resultado sería 103.1733, con `ceiling()` lo que haces es redondear al alza, por lo que `ceiling(37690/375)` dará como resultado 104 y tendrás una página 104 en la que guardará el resto. Si quisiereas redondear a la baja usarías la función `floor()`. Escribe en la consola:

```{R}
# Distintos resultados de la misma división
# Sin redondeo
38690/375
# Redondeo a la baja
floor(38690/375)
# Redondeo al alza
ceiling(38690/375)
```

El resultado de todo esto es que en `trozos` tienes una lista con tantos vectores como se hayan calculado con `ceiling()` y que corresponde a cada una de las páginas en las que has dividido el texto que has leído. Ya has acabado con la primera parte del bucle. Tómate un descanso.

Vamos con el segundo bucle, el anidado cuyo número de iteraciones está controlado por la longitud de la lista `trozos` que has conseguido en el bucle exterior y cuya variable de control es `i` (¡cuidado! no las enredes), en la que vuelves a unir las palabras de cada trozo en una página que vas a guardar `ensayos`.

En cada iteración de `i`, extraerás un vector de `trozos` y lo guardarás en `fragmento`. En la siguiente línea convertirás la lista –`unlist()`– que has sacado de trozos en una cadena de caracteres `paste(unlist(fragmento), collapse = " "))`. Sí, ya sé que es de locos. Unir, cortar, unir… ¡vaya rollo!

Lo que acabas de unir, lo guardarás en la columna `texto` de `fragmento.unido`, que es una tabla provisional en la que crearás cada una de las páginas de cada uno de los textos. La información que tendrá cada tabla es el `texto`, el identificador con `titulo` y el número de `pagina`. Fíjate bien que el número de `pagina` es el valor de `i`, es decir, el número de `trozo`, mientras que el identificador que guardarás en `titulo` es el contenido `j` del vector `titulos`, es decir, el título lo controla el bucle externo y que es el único que sabe cuántas vueltas dará, porque es igual al número de fichero leídos y que tienes guardado en el vector `ficheros`. Para que veas cántas vueltas dará escribe en la consola

```{R}
length(ficheros)
```

La última línea del bucle es una vieja conocida. Lo que hace es añadir a la tabla `ensayos` que creaste antes de comenzar el bucle cada uno de los trozos reunidos, es decir, cada una de las páginas que encontraste en aquel polvoriento legajo. Ahora procederás a borrar todos los objetos intermedios que has creado para dividir los textos en páginas. Podrías haberlo hecho a mano, pero te habría llevado mucho tiempo.

```{R}
rm(ficheros, titulos, trozos, fragmento, fragmento.unido, url, texto.entrada, texto.palabras, texto.todo, por.palabras, i, j)
```

Ahora solo debe haber en **Environment** dos objetos: `ensayos` y `vacias`. Es decir, una tabla con todas las páginas y otra con las palabras vacías que borrarás para hallar los tópicos que puede haber en estos textos y ver si el ordenador es capaz de unir las páginas (no digo ordenar, digo juntar temáticamente).

### Continuamos…

`ensayos` es una tabla de 364 observaciones (filas) y tres variables (columnas): `texto`, `titulo` y `pagina` en las que has guardado todas las páginas del famoso legajo. La columna `titulo` la he añadido para que sepamos después si el sistema ha funcionado o no, podría funcionar sin ello, pero para aprender es mejor incorporar esa columna. El primer paso, como de costumbre, es dividirlo en palabras-token guardarlas en una tabla que llamarás `por_pagina_palabras`. Pero para saber de qué folio es cada palabra, vas a crear una clave, que guardarás en la columna `titulo_pagina`, a base de unir con `unite()` los valores de `titulo` y `pagina`.

```{R}
por_pagina_palabras <- ensayos %>%
  unite(titulo_pagina, titulo, pagina) %>%
  unnest_tokens(palabra, texto)
```

El resultado de lo anterior lo que puedes ver si escribes en la consola

```{R eval = FALSE}
por_pagina_palabras
```

para ver el resultado, que es:


```{R echo = FALSE}
por_pagina_palabras
```

Como puedes ver, se trata de una tabla con 135905 palabras identificadas cada una por el título y el número de la página. Como te he dicho, podría bastar con el número de página, pero para comprobar el resultado le he añadido el nombre del pensador que se analiza.

Échale ahora una ojeada a las palabras-tipo más frecuentes con

```{R}
por_pagina_palabras %>%
  count(palabra, sort = T)
```

Algo absolutamente decepcionante. En algunos análisis, como has visto, y como verás, son interesantes las palabras de función, pero en otros son un estorbo porque introducen ruido indeseado. Así que hay que borrarlas con `anti_join()`. Vas a crear una nueva lista de palabras sin esas palabras indeseadas y la guardarás en una nueva tabla que llamarás `palabras_conteo`. En esta tabla, además de la `palabra` y la clave `titulo_pagina`, guardarás el número de veces que aparece cada una de ellas en cada página en la columna (variable) `n`. Lo consigues con

```{R eval = FALSE}
palabra_conteo <- por_pagina_palabras %>%
  anti_join(vacias) %>% 
  count(titulo_pagina, palabra, sort = TRUE) %>%
  ungroup()
```

Recuerda que aparecerá un mensaje informándote que la `anti_join()` ha usado la variable común `palabra`

```{R echo = FALSE}
palabra_conteo <- por_pagina_palabras %>%
  anti_join(vacias) %>% 
  count(titulo_pagina, palabra, sort = TRUE) %>%
  ungroup()
```

Puedes ver cuántas palabras de valor `palabra` hay en cada folio del legajo (cada unos de ellos está identificado con `titulo_numero`) y cuál es su frecuencia `n` con

```{R eval = FALSE}
palabra_conteo
```
que imprimirá esta tabla:

```{R echo = FALSE}
palabra_conteo
```


## `Document Term Matrix`
Lo que tienes es una tabla con un término por documento en cada fila. Sin embargo, el paquete `topicmodels` utiliza otro tipo de estructura, la llamada `Document-Term Matrix` (DTM) procedente del paquete `tm`. No te voy a introducir en las sutilezas de esta matriz, pero de una forma sencilla es una gran tabla en la que en cada fila hay un documento y en cada columna hay una palabra. Considera estas sencillas frases:

```
F1 Hace mucho frío
F2 Hace mucho calor
F3 Hace mucho tiempo
```

Si las convertimos en una `Document-Term Matrix` tendría este aspecto:

||Hace|mucho|frio|calor|tiempo|
|---|:---:|:---:|:---:|:---:|:---:|
|F1|1|1|1|0|0|
|F2|1|1|0|1|0|
|F3|1|1|0|0|1|

Esta matriz te muestra qué documento (frases en este caso: F1, F2, F3) tiene qué palabras y cuántas veces.

La forma de obtener este tipo de matriz, que es con la que opera `topicmodels`, es muy sencillo con la función `cast_dtm()` del paquete `tidytext`, del ecosistema `tidydata` que estás utilizando. Este nuevo objeto matriz lo vas a llamar `paginas_dtm`, así tienes claro qué tipo de objeto es. Es una orden muy simple.  La función `cast_dtm()` solo necesita saber de qué objeto ha de extraer los datos, en este caso `palabras_conteo`, y cuáles son las variables que ha de recuperar: la referencia del documento `titulo_pagina`, las palabras `palabra` y las frecuencias `n`, de ahí que crearas la tabla `palabras_conteo`. Transformarla en una `DTM` es tan sencillo como

```{R}
paginas_dtm <- palabra_conteo %>%
  cast_dtm(titulo_pagina, palabra, n)
```
Para revisar ahora el resultado, escribe

```{R eval = FALSE}
paginas_dtm
```

que te ofrecerá este mensaje:

```{R echo = FALSE}
paginas_dtm
```

La primera línea informa de que en `paginas_dtm` hay 364 documentos (páginas en tu caso) y que hay 15900 términos diferentes en total. En definitiva, lo que tienes es una tabla con 5732358 celdas cuyo valor puede ser 0, no aparece en el documento, o un valor superior a 0 (los informáticos dicen non-zero), por lo que en la segunda línea te informa de que 5732358 celdas tienen como valor 0 y tan solo 55242 tiene un valor mayor que 0, lo que supone que el 99 % de las filas (tercera línea) tienen como valor 0. La cuarta línea te dice que la palabra más larga tiene 20 caracteres y que la forma de considerar los términos es por medio de su frecuencia (`term frquency` o `tf`).

## Latent Dirichlet Allocation
Ahora estás listo para usar el paquete `topicmodels` y construir un modelo `LDA` (Latent Dirichlet allocation). Pero antes, conviene que tengas una ligera noción de como funciona el modelo LDA y quién lo creó, para lo que me baso en un post de [José Calvo](http://www.morethanbooks.eu/topic-modeling-introduccion/) sobre el modelado de tópicos.

El autor más influyente sobre topic modeling es sin duda David Blei, quien, en un artículo de 2012 titulado [«Probabilistic Topic Models»](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf), propuso un modelo sobre _topic modeling_ llamado `Latent Dirichlet allocation`. Desde entonces este modelo es el más utilizado. La imagen de la figura anterior se utiliza en casi cualquier presentación sobre _topic modeling_.

```{R 11-Sentimiento-01, echo = FALSE, out.width = '75%', fig.align='center', fig.cap = "Topic Modeling según la visión de [Blei](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)"}
knitr::include_graphics("imagenes/09-Topic-LDA.png")
```

En el artículo en el que Blei presenta LDA, define _topic_ como «a distribution over a fixed vocabulary» y su punto de partida es intentar recrear de manera inversa el modelo teórico por el que los textos se generan. Según él, los autores tienen a su disposición un conjunto delimitado y cerrado de temas o topics (en la figura anterior son las listas de palabras con fondos de diferentes colores), cada uno de esos topics contiene palabras (algunas más importantes para el topic, otras menos), y el autor va sacando palabras de los diferentes _topics_ para escribir su texto (en la parte derecha de la imagen superior, algunas palabras tienen fondos de diferentes colores señalando su pertenencia al _topic_). Para rehacer de manera inversa ese proceso generativo hipotético, Blei realiza los siguientes pasos teóricos:

1. Selecciona aleatoriamente una distribución de los tópicos
2. Para cada palabra del documento
    a. Selecciona aleatoriamente un tópico de la distribución de tópicos del paso 1.
    b. Selecciona aleatoriamente una palabra de la distribución correspondiente del vocabulario.

Como puedes observar, el adverbio _aleatoriamente_ está en todos los pasos. Como es de esperar, esto provoca que cada vez que utilices el _topic modeling_ sobre un corpus, los resultados muestren cierta variación, es decir, pueden ofrecer resultados semejantes pero no idénticos.

Quizás ahora mismo estés pensando: 1) los textos no se generan así; 2) no entiendo eso de «a distribution over a fixed vocabulary»; 3) ¿cómo puede ser que todos los pasos sean aleatorios? Si estás pensando eso, ¡enhorabuena!, es el momento de maravillarte de que un método tan ilógico y oscuro funcione tan bien como para extraer que _profesor_, _maestro_, _libro_ y _escuela_ están relacionados.

### Construir el modelo LDA
El modelado de tópicos en gran medida se puede resumir en dos puntos:

1. Todo documento es una mezcla de tópicos.
2. Cada tópico es una mezcla de palabras.

El `LDA` es un modelo matemático que sirve para estimar estos dos puntos a la vez: localizar la mezcla de palabras que se asocia con cada tópico y, a la vez, determinar la mezcla de tópicos que sirven para describir cada documento.

Para establecer el modelo se utiliza la función `LDA()` de la librería `topicmodels` que requiere tres argumentos: el objeto sobre el que se construirá el modelo –`paginas_dtm`–, el número de tópicos –`k`– y un tercer argumento –`control = list()`– que sirve para hacer el modelo predecible. El resultado de este cálculo lo guardarás en `paginas_lda`.  La línea de código es:

```{R}
paginas_lda <- LDA(paginas_dtm, k = 4, control = list(seed = 1234))
```

El valor de `k` se ha establecido porque partes del conocimiento previo de que hay cuatro pensadores (`filosofo1` a `filosofo4`), pero la verdad es que en este tipo de análisis hay que juguetear con los posibles valores. Con `seed = 1234` dentro `list()` lo que se logra es un punto de comienzo fijo para el proceso de iteración aleatoria. Si no se hiciera así, cada vez que se ejecute el script se estimarían modelos ligeramente diferentes.

Una vez ejecutada la orden anterior, el resultado lo puedes ver con

```{R eval = FALSE}
paginas_lda
```

no será muy excitante, incluso será impenetrable e inescrutable. 

```{R echo = FALSE}
paginas_lda
```

Esto lo que quiere decir es que se ha creado un modelo con cuatro tópicos y que ha utilizado el algoritmo `VEM` (= _Variational Expectation Maximization_), de lo cual vamos a pasar porque las matemáticas en las que se basan son realmente enredadas. Si quieres echarle una ojeada, aquí tienes el manual de [`topicmodels`](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf).

Lo cierto es que ya tienes el modelo de tópicos construido, pero hay que extraerlo de `paginas_lda` para que sea compatible con el ecosistema `tidydata` que estás usando. Lo primero que hay que ver son las probabilidades de palabras que hay en cada tópico. De nuevo, las cuestiones matemáticas son complejas, por lo que me limitaré a ir dando los pasos y explicando qué es lo que se hace en cada uno de ellos.

Lo primero es calcular las probabilidades de que una palabra pertenezca uno u otro tópico, pero lo queremos en el formato que ya conocemos, por lo que hay que convertirlo con la función `tidy()` que construye una tabla (`tibble`) que resume todos los hallazgos estadísticos del modelo. Esas matemáticas son incomprensibles para el común de los mortales, pero realizan la magia que necesitas.

```{R}
paginas_lda_td <- tidy(paginas_lda, matrix = "beta")
```

Ahora, cuando escribas en la consola

```{R eval = FALSE}
paginas_lda_td
```

obtendrás una vieja conocida:

```{R echo = FALSE}
paginas_lda_td
```

es decir, tienes tres columnas. La primera, `topic`, indica el número de tópico; la segunda ofrece las palabras –`term`– que aparecen en el conjunto de datos y, por último, la columna `beta` informa de la la probabilidad de que el término haya sido generado por el tópico. Cuanto más cerca esté de 1 un valor, tanto más probable es que una palabra –`term`– sea parte del tópico –`topic`–. Pero… te lo ha presentado en notación científica que, si bien es cómoda entre los matemáticos, al resto de la humanidad le cuesta un poco.

Esta notación la puedes interpretar de la siguiente manera: Escribe tantos ceros a la izquierda como te diga el número de la derecha y entre los dos últimos ceros de la izquierda pon el punto decimal. Así, la primera ocurrencia de _lenguaje_, que tiene un valor de 7.67e-5, lo que tienes que escribir es cinco ceros a la izquierda de 7.67 y mover el punto hasta antes del último cero, el resultado será 0.0000767. Así, _lenguaje_, con una probabilidad de 0.0127 (1.27e-2) es mucho más probable que la haya generado el tópico 4 que los tópicos 1, 2 o 3. Cuantos más ceros haya entre el punto decimal y el primer dígito superior a 0, menor es la probabilidad. Lo que parece seguro es que el tópico 3 no ha podido generar el término _lenguaje_, su probabilidad tiene 312 ceros a la izquierda. _estructura_ es más probable que sea del tópico 4 (0.00422) que de cualquiera de los otros tres (0.0000955, 0.000606 y 0.0000948).

Vas a ver los cinco primeros términos más probables para cada tópico, pero para que puedas verlo con números más fáciles, ejecuta esta orden:

```{R}
options(scipen=999)
```

Esto hará que en la mayoría los casos los números muy pequeños se impriman con todos los ceros pertinentes, aunque ya te he dado una buena pista de cómo solucionar el problema, si es que lo es.  Como te decía, vas a ver los cinco primeros términos más probables para cada tópico y los vas a guardar en en una tabla llamada `terminos_frecuentes`:

```{R}
terminos_frecuentes <- paginas_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```

Lo que acabas de hacer ha sido extraer de `paginas_lda_td` los datos, agrupándolos –`group_by()`– por `topic` (tópico). Le pides que solo extraiga los cinco más probables para cada tópico `top_n()`, lo cual viene determinado por el valor de `beta`.  Que los desagrupe –`ungroup()`– y ordene –`arrange()`– los tópicos de mayor a menor probabilidad, de ahí el argumento `-beta`. El resultado lo puedes ver con

```{R eval = FALSE}
terminos_frecuentes
```

que imprirá esta tabla:

```{R echo = FALSE}
terminos_frecuentes
```

Una simple ojeada a la tabla puede permitirte establecer que el tópico 1 tiene que ver con Voltarie, que el 2 con Freud, el 3 con Maquiavelo y el 4 con Chomsky. No solo porque aparezcan los nombres de estos cuatro pensadores en cada uno de ellos, sino por algunos de los términos: en el tópico 4 todos han podido ser generados por un texto que habla de Chomsky; los términos _príncipe_, _política_ y _secretario_ del tópico 3 se pueden asociar con Maquiavelo; _psicoanálisis_ del tópico 2 con Freud, etc. Sin embargo, estos datos es más fácil verlos con una gráfica en la que puedes ver los cinco términos (`term`) y la probabilidad más alta (`beta`) de que hayan sido generados por cada uno de los cuatro tópicos.

La gráfica se consigue con estas líneas de código, que a estas alturas ya deben serte claras:

```{R}
terminos_frecuentes %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

Un punto importante en todo este proceso es que la función `LDA()` no etiqueta, es decir, no identifica cada uno de los tópicos. Son sencillamente los tópicos 1, 2, 3 y 4. A partir de esto, puedes inferir que están asociados con cada uno de los textos. Pero ten en cuenta que es una mera inferencia personal.


### Clasificación por documento
Cada uno de los folios del supuesto legajo es un documento. Por lo tanto, es hora de conocer qué tópicos se asocian con cada folio (documento). La pregunta es ¿podrías agrupar los folios que hablan de cada uno de los filósofos y crear cuatro montones? No estoy diciendo que los ordene secuencialmente, tan solo que los apile. La máquina puede hacer muchas cosas, pero no tantas.

Hace un rato usaste la función `tidy()` para calcular la probabilidad de que un término procediera de un tópico u otro. Ahora la vuelves a emplear para calcular la probabilidad de que una página participe de uno u otro tópico y puedas establecer si es sobre uno u otro pensador. La función es prácticamente la misma, lo único que cambia es el valor del argumento `matrix`, que pasa a ser `gamma`. Así que

```{R}
paginas_lda_gamma <- tidy(paginas_lda, matrix = "gamma")
```

Puedes examinar el contenido de esta nueva tabla con

```{R eval = FALSE}
paginas_lda_gamma
```

lo que imprimirá este resultado:

```{R echo = FALSE}
paginas_lda_gamma
```

Puedes ver que, entre las 10 primeras páginas (`document`) de las 1456 que constituyen el corpus, la única que tienen una probabilidad de 1 (puede ser 1 –altísima– o 0 –bajísima–) de que esas páginas procedan del tópico 1, es `Voltaire_21` mientras que las demás están bastante alejadas. Es el momento de ver cuántas páginas de cada filósofo están dentro del tópico que ha establecido el modelo, que es lo que puedes ver con un gráfico, que es mucho más cómodo que una larguísima tabla de números.

### Gráfico de los tópicos

Pero antes de poder trazar el gráfico tienes que separar los nombres de las páginas para poder etiquetar los tópicos con los nombres de los filósofos y no con el indescifrable 1, 2, 3 y 4. Lo consigues con

```{R}
paginas_lda_gamma <- paginas_lda_gamma %>%
  separate(document,
           c("titulo", "pagina"),
           sep = "_", convert = TRUE)
```

Lo que le estás diciendo a `R` es que en la misma tabla `paginas_lda_gamma` separe en dos columnas nuevas –`titulo` y `pagina`– el contenido de la columna `document` y que la frontera entre los dos elementos es un guion bajo `sep="_"`. El contenido de `paginas_lda_gamma` es semejante al que has visto en la tabla de resultados anterior, solo que esta vez ha desaparecido la columna `document` y en su lugar están `titulo`, que contiene el nombre del pensador, y `pagina`, que recoge el número de la misma. Puedes ver con

```{R eval = FALSE}
paginas_lda_gamma
```

que mostrará el comienzo de la tabla:

```{R echo = FALSE}
paginas_lda_gamma
```

De nuevo, leer largas tablas de números es muy poco iluminador, por lo que hay que ofrecer los datos visualmente, por medio de un histograma o gráfica de barras. Para ello usas esta sencilla orden de `ggplot()`

```{R eval = FALSE}
ggplot(paginas_lda_gamma, aes(gamma, fill = factor(topic))) +
  geom_histogram() +
  facet_wrap(~ titulo, nrow = 2)
```

Al ejecutarlo aparecerá en la consola el siguiente mensaje. No tienes que preocuparte de nada. Es un mero aviso de `R`. En el panel **plots** se dibujará la gráfica.

```{R echo = FALSE}
ggplot(paginas_lda_gamma, aes(gamma, fill = factor(topic))) +
  geom_histogram() +
  facet_wrap(~ titulo, nrow = 2)
```

Como puedes ver, en todos los casos la mayoría de las páginas se asocian con un único tópico. En Voltaire es el que más páginas tienen problemas de asignación, aunque la mayor parte de los problemas son en realidad una baja estimación (probabilidad) de que las páginas sean del tópico 1 sean de un texto sobre Voltaire. Pero hay numerosas páginas tienes sus pequeños problemas de asociación con un u otro tópico. Puedes averiguar qué tópico se asocia mejor con qué página con `top_n()`, lo cual que ofrecerá la clasificación otorgada a esa página. Copia el siguiente bloque de código y ejecútalo

```{R}
paginas_clasificaciones <- paginas_lda_gamma %>%
  group_by(titulo, pagina) %>%
  top_n(1, gamma) %>%
  ungroup() %>%
  arrange(gamma)
```

Lo que has creado es una nueva tabla llamada `paginas_clasificaciones` con los datos de `paginas_lda_gamma`. Primero agrupas los datos por título y página con `group_by()`; le pides que seleccione la máxima con `top_n()` clasificación para cada página de acuerdo con el cálculo de `gamma`; que deshaga la agrupación con `ungroup()` porque ya no tiene interés; y, por último reorganice `arrange()` los resultado de acuerdo con el valor de `gamma`. Para ver los resultados de la ejecución de la orden anterior escribe

```{R eval = FALSE}
paginas_clasificaciones
```

lo que imprirá la tabla:

```{R echo = FALSE}
paginas_clasificaciones
```

Ahora puedes establecer el tópico de consenso para cada pensador, es decir, el tópico más común para cada una de sus páginas y ver cuáles son las páginas que se han clasificado erróneamente. Para ello necesitas crear una nueva tabla con los tópicos de cada pensador que llamarás `topico_pensador`. Es muy parecido al tipo de clasificación que lograste en el paso anterior, solo que esta vez creas una nueva columna con `transmute()` llamada `consenso` que combina los valores de `titulo` y `topic` que te permite ver, textualmente, lo que viste gráficamente. Esta confirmación la consigues con

```{R}
topico_pensador <- paginas_clasificaciones %>%
  count(titulo, topic) %>%
  group_by(titulo) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consenso = titulo, topic)
```

Una vez ejecutado lo anterior, para ver el resultado escribe y ejecuta en la consola

```{R eval = FALSE}
topico_pensador
```

lo te presentará este resultado

```{R echo = FALSE}
topico_pensador
```

Es decir, el tópico 4 corresponde a Chomsky, el 3 a Maquiavelo, el 2 a Freud y el 1 a Voltaire.

También puedes ver cuáles son las páginas erróneamente asignadas con

```{R eval = FALSE}
paginas_clasificaciones %>%
  inner_join(topico_pensador, by = "topic") %>%
  filter(titulo != consenso)
```

La orden anterior lo que hace es extraer de `paginas_clasificaciones` aquellas páginas que el modelo no asignó correctamente. Para ello, añadirá con `inner_join()` la columna `consenso` de `topico_pensador` por medio del valor de la columna `topic` y después extraerá con `filter()` todas aquellas filas en la que `titulo` y `consenso` no son iguales `!=`. El resultado es:


```{R echo = FALSE}
paginas_clasificaciones %>%
  inner_join(topico_pensador, by = "topic") %>%
  filter(titulo != consenso)
```

Hay 54 páginas que el modelo no fue capaz de asignar correctamente. Quizá sea un pequeño fracaso, hay casi un error del 15 %. Pero ten en cuenta que la máquina no sabe nada de filosofía ni quiénes eran estos pensadores. Recuerda que lo que has usado ha sido un sistema no supervisado, es decir, has dejado a la máquina que tome todas las decisiones. Por lo tanto, no está tan mal.

## Asignaciones por palabras
Uno de los pasos que da LDA es asignar cada una de las palabras de cada documento, página en tu caso, a un tópico. Por lo tanto, cuantas más palabras de una página sean asignadas a un tópico, el peso (`gamma`) será mayor para establecer la clasificación. Puedes averiguar qué palabras de cada página asignó el algoritmo a cada tópico.

De nuevo tienes que extraer datos de un objeto peculiar `paginas_lda`. Para ello usarás la función `augment()`, es similar a la `tidy()` que usaste antes, pero esta vez lo que ocurrirá es que utilizará el modelo para añadir información a los datos originales que tienes guardados en `paginas_dtm`, aquella enorme matriz que creaste y cuyo contenido era una enorme tabla de 364 filas y 15900 columnas, es decir, 5732358 casillas. Empieza por crear la tabla de las asignaciones:

```{R}
asignaciones <- augment(paginas_lda, data = paginas_dtm)
```

Una vez creada, revisa su contenido con

```{R eval = FALSE}
asignaciones
```

que te ofrecerá este resultado:

```{R echo = FALSE}
asignaciones
```

Esta nueva tabla contiene el recuento de los términos de cada página y ha añadido una nueva columna, `.topic`, que indica al tópico a que se ha asignado el palabra (`term`) dentro de cada página (`document`). Una vez que tienes esta tabla, puedes combinarla con la tabla de consenso que creaste en `topico_pensador` para ver qué palabras son las que no ha logrado asignar correctamente:

```{R}
asignaciones <- asignaciones %>%
  separate(document, c("titulo", "pagina"),
           convert = TRUE) %>%
  inner_join(topico_pensador,
             by = c(".topic" = "topic"))
```

aunque el resultado, que ves puede parecerte absurdo, pues no ves datos que te permitan ver cuáles son las palabras que el modelo asignó erróneamente. Revísalo con

```{R eval = FALSE}
asignaciones
```

que te ofrecerá esta tabla:

```{R echo = FALSE}
asignaciones
```

Lo que en realidad has conseguido es añadir separar el nombre del pensador y el número de la página, algo que ya hiciste hace rato, y has añadido la columna `consenso` basándote en el valor de `topic`. Bueno, puedes utilizar esta información para dibujar una matriz de confusión, una gráfica interesante que permite visualizar con rapidez qué tal funcionó el modelo.

Como de costumbre, la orden para dibujarla es compleja, pero bastante clara a estas alturas del curso:
```{R}
asignaciones %>%
  count(titulo, consenso, wt = count) %>%
  group_by(titulo) %>%
  mutate(porcentaje = n / sum(n)) %>%
  ggplot(aes(consenso, titulo, fill = porcentaje)) +
  geom_tile() +
  scale_fill_gradient2(high = "blue", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Asignó las palabras a…",
       y = "Las palabras procedían de…",
       fill = "% de asignaciones")
```
Como puedes ver en el gráfico anterior, la mayoría de las palabras referidas a Chomsky, Maquiavelo y Freud están correctamente asignadas. Es Voltaire el que concentra el mayor número de palabras equivocadas. ¿Cuáles son esas palabras? Puedes hallarlas con facilidad. Tan solo tienes que localizar en `asignaciones` qué palabras no coinciden en el `titulo` y el `consenso`

```{R}
palabras_equivocadas <- asignaciones %>%
  filter(titulo != consenso)
```

El resultado es una tabla con 7.484 términos. Échale una ojeada con

```{R eval = FALSE}
palabras_equivocadas
```

lo que imprirá en la consola:

```{R echo = FALSE}
palabras_equivocadas
```

Como puedes, hay una serie de palabra que a menudo las ha asignado a Chomsky y Maquiavelo, aunque también aparecen en Voltaire. Por otra parte, _lenguaje_ y _estructura_ son palabras que el modelo ha asignado a Chomsky porque son mucho más corrientes entre las páginas sobre Chomsky, lo que se puede confirmar con una simple revisión del conteo de las palabras como este

```{R eval = FALSE}
palabras_equivocadas %>%
  count(titulo, consenso, term, wt = count) %>%
  ungroup() %>%
  arrange(desc(n))
```

que te ofrcerá una tabla como esta:

```{R echo = FALSE}
palabras_equivocadas %>%
  count(titulo, consenso, term, wt = count) %>%
  ungroup() %>%
  arrange(desc(n))
```

Ten en cuenta que el modelo es estocástico e iterativo, es decir, aleatorio y repetitivo, por lo cual no debe sorprenderte que tenga problemas cuando un tópico aparece en varios textos (documentos) a la vez.

## Limitaciones del topic modeling
Según [C. Bail](https://cbail.github.io/SICSS_Topic_Modeling.html) el _topic modeling_ se han convertido en una herramienta estándar dentro del análisis de texto cuantitativo por muchas razones. El modelado de tópicos puede ser mucho más útil que los análisis basados en la frecuencia de palabras simples o en diccionarios, depende del caso. El modelado de tópicos tiende a producir los mejores resultados cuando se aplica a textos que no son demasiado cortos, y aquellos que tienen una estructura consistente, como es caso de textos literarios o ensayísticos.

Pero, al mismo tiempo, el _topic modeling_ tiene una serie de limitaciones. Para comenzar, el mismo término _topic_ es equívoco y, por el momento, el _topic modeling_ no puede ofrecer una clasificación de los textos altamente refinada. Por otra parte, el modelado de tópicos puede servir para ofrecer una interpretación errada del significado de un texto. Estas herramientas es mejor considerarlas como ayudas a la lectura. Los resultados de un modelado de tópicos no deben ser interpretados en demasía a menos que el investigador tenga una fuerte base teórica acerca de los tópicos de un corpus dado o si ha validado cuidadosamente los resultados, tanto cuantitativa como cualitativamente.

-----
#### Práctica 9.1.

Esta manera de abordar el topic modeling está basada y desarrollada en el modelo desarrollado por [Robinson y Silge](https://www.tidytextmining.com/topicmodeling.html). En su presentación usan cuatro obras literarias extensas: _20.000 leguas de viaje submarino_, _Grandes esperanzas_, _La guerra de los mundos_ y _Orgullo y prejuicio_. Buscaron cuatro obras de temática muy dispar para evitar los _fallos_. Te propongo que reescribas las porciones adecuadas del script que hay a lo largo de las explicaciones precedentes para que puedas analizar estas cuatro obras. Las puedes bajar del repositorio del libro. Puesto que no se trata de dividir en "páginas" sino en capítulos, te ofrezco el comienzo del script hasta la división en capítulos:

```{R eval = FALSE}
# Carga las librerías
library(tidyverse)
library(tidytext)
library(tm)
library(topicmodels)
library(scales)
# Evita la notación científica
options(scipen=999)
# Carga la lista de palabras vacías
# Carga la lista de stopwords españolas
vacias <- as_tibble(read_delim("https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/diccionarios/vacias.txt",
                               delim = "\t",
                               col_names = TRUE,
                               quote = "\"",
                               locale = default_locale()))
# Los textos los cargas desde un repositorio externo
url <- "https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/textos/"
# Localiza los textos
titulos <- c("20000 leguas de viaje submarino", "Grandes esperanzas", "La guerra de los mundos", "Orgullo y Prejuicio")
ficheros <- c("20000LeguasViajeSubmarino.txt", "GrandesEsperanzas.txt", "LaGuerraDeLosMundos.txt", "OrgulloPrejuicio.txt")
# Crea la tabla en que guardará todo los textos
novelas <- tibble(texto = character(),
                  titulo = character())
# Lee los textos
for (i in 1:length(titulos)) {
  texto <- read_lines(paste(url, ficheros[i], sep = ""))
  temporal <- tibble(texto = texto, titulo = titulos[i])
  novelas <- bind_rows(novelas, temporal)
}
# Los divide por capítulos
por_capitulo <- novelas %>%
  group_by(titulo) %>%
  mutate(capitulo = cumsum(str_detect(texto,
                                      regex("^cap[í|i]tulo ",
                                            ignore_case = TRUE)))) %>% 
  ungroup() %>%
  filter(capitulo > 0)
# Una mirada al resultado parcial…
por_capitulo
# Aquí sigues tú…
```

Los tres gráficos que has de obtener se deben parecer a los que tienes a continuación.

```{R echo = FALSE, message = FALSE}
library(tidyverse)
library(tidytext) 
library(tm)
library(topicmodels)
library(scales)

# Evita la notación científica
options(scipen=999)
# Carga la lista de stopwords españolas ya que tidytext solo las tiene en inglés
vacias <- as_tibble(read.delim("https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/diccionarios/vacias.txt",
                               header= TRUE,
                               quote = '"',
                               encoding = "UTF-8",
                               stringsAsFactors = F))

# Localiza los textos
titulos <- c("20000 leguas de viaje submarino", "Grandes esperanzas", "La guerra de los mundos", "Orgullo y Prejuicio")
ficheros <- c("20000LeguasViajeSubmarino.txt", "GrandesEsperanzas.txt", "LaGuerraDeLosMundos.txt", "OrgulloPrejuicio.txt")
url <- "https://raw.githubusercontent.com/7PartidasDigital/AnaText/master/datos/textos/"
# Crea la tabla en que guardará todo los textos
novelas <- tibble(texto = character(),
                      titulo = character())
# Lee los textos
for (i in 1:length(titulos)) {
  texto <- read_lines(paste(url, ficheros[i], sep = ""), locale = default_locale())
  temporal <- tibble(texto = texto, titulo = titulos[i])
  novelas <- bind_rows(novelas, temporal)
}
por_capitulo <- novelas %>%
  group_by(titulo) %>%   
  mutate(capitulo = cumsum(str_detect(texto, regex("^cap[í|i]tulo ", ignore_case = TRUE)))) %>%   
  ungroup() %>%   
  filter(capitulo > 0)
por_capitulo_palabras <- por_capitulo %>%   
  unite(titulo_capitulo, titulo, capitulo) %>%                 
  unnest_tokens(palabra, texto)
palabra_conteo <- por_capitulo_palabras %>%   
  anti_join(vacias) %>%   
  count(titulo_capitulo, palabra, sort = TRUE) %>%   
  ungroup()
capitulos_dtm <- palabra_conteo %>%
  cast_dtm(titulo_capitulo, palabra, n)
capitulos_lda <- LDA(capitulos_dtm, k = 4, control = list(seed = 1234)) # Ojo al valor de k
capitulos_lda_td <- tidy(capitulos_lda, matrix = "beta")
terminos_frecuentes <- capitulos_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
terminos_frecuentes %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
capitulos_lda_gamma <- tidy(capitulos_lda, matrix = "gamma")
capitulos_lda_gamma <- capitulos_lda_gamma %>%
  separate(document, c("titulo", "capitulo"), sep = "_", convert = TRUE)
ggplot(capitulos_lda_gamma, aes(gamma, fill = factor(topic))) +
  geom_histogram() +
  facet_wrap(~ titulo, nrow = 2)
capitulo_clasificaciones <- capitulos_lda_gamma %>%
  group_by(titulo, capitulo) %>%
  top_n(1, gamma) %>%
  ungroup() %>%
  arrange(gamma)
topicos_libro <- capitulo_clasificaciones %>%
  group_by(titulo) %>%
  count(titulo, topic) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consenso = titulo, topic)
asignaciones <- augment(capitulos_lda, data = capitulos_dtm)
asignaciones <- asignaciones %>%
  separate(document, c("titulo", "capitulo"), sep = "_", convert = TRUE) %>%
  inner_join(topicos_libro, by = c(".topic" = "topic"))
asignaciones %>%
  count(titulo, consenso, wt = count) %>%
  group_by(titulo) %>%
  mutate(porcentaje = n / sum(n)) %>%
  ggplot(aes(consenso, titulo, fill = porcentaje)) +
  geom_tile() +
  scale_fill_gradient2(high = "blue", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "las asigno a…",
       y = "Las palabras procedían de…",
       fill = "% de asignaciones")

```

¿Qué conclusiones extraes?


[^1]: En R hay otras librerías que pueden realizar el modelado de tópicos. Una muy popular entre los especialistas en literatura es `mallet`, de David Mimmo. Originalmente se escribió en Java, lo que implica que lo puedes utilizar fuera de `R` (Programming Historian tiene un interesante [tutorial en español](https://programminghistorian.org/es/lecciones/topic-modeling-y-mallet)), pero para usarlo en el entorno de `R`, que es el objetivo de este curso, requiere instalar `rJava`, lo que puede ser una pesadilla en los ordenadores Apple. Debido a esto y, a que no hace uso del ecosistema `tidydata` que estamos empleando, prefiero recurrir a `topicmodels`.
